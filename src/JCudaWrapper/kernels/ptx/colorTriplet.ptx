//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-32267302
// Cuda compilation tools, release 12.0, V12.0.140
// Based on NVVM 7.0.1
//

.version 8.0
.target sm_52
.address_size 64

	// .globl	colorTripletKernel

.visible .entry colorTripletKernel(
	.param .u32 colorTripletKernel_param_0,
	.param .u64 colorTripletKernel_param_1,
	.param .u32 colorTripletKernel_param_2,
	.param .u64 colorTripletKernel_param_3,
	.param .u32 colorTripletKernel_param_4,
	.param .u64 colorTripletKernel_param_5,
	.param .u32 colorTripletKernel_param_6
)
{
	.reg .pred 	%p<21>;
	.reg .b32 	%r<66>;
	.reg .f64 	%fd<56>;
	.reg .b64 	%rd<13>;


	ld.param.u32 	%r5, [colorTripletKernel_param_0];
	ld.param.u64 	%rd2, [colorTripletKernel_param_1];
	ld.param.u32 	%r2, [colorTripletKernel_param_2];
	ld.param.u64 	%rd3, [colorTripletKernel_param_3];
	ld.param.u32 	%r3, [colorTripletKernel_param_4];
	ld.param.u64 	%rd4, [colorTripletKernel_param_5];
	ld.param.u32 	%r4, [colorTripletKernel_param_6];
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r1, %r7, %r6, %r8;
	setp.ge.s32 	%p1, %r1, %r5;
	@%p1 bra 	$L__BB0_15;

	cvta.to.global.u64 	%rd5, %rd2;
	mul.lo.s32 	%r9, %r1, %r2;
	mul.wide.s32 	%rd6, %r9, 8;
	add.s64 	%rd7, %rd5, %rd6;
	mul.lo.s32 	%r10, %r1, %r3;
	cvta.to.global.u64 	%rd8, %rd3;
	mul.wide.s32 	%rd9, %r10, 4;
	add.s64 	%rd1, %rd8, %rd9;
	ld.global.f64 	%fd1, [%rd7];
	setp.ge.f64 	%p2, %fd1, 0d0000000000000000;
	setp.lt.f64 	%p3, %fd1, 0d3FF0C152382D7365;
	and.pred  	%p4, %p2, %p3;
	@%p4 bra 	$L__BB0_12;
	bra.uni 	$L__BB0_2;

$L__BB0_12:
	mov.u32 	%r51, 255;
	st.global.u32 	[%rd1], %r51;
	mul.f64 	%fd42, %fd1, 0d4008000000000000;
	div.rn.f64 	%fd43, %fd42, 0d400921FB54442D18;
	mul.f64 	%fd44, %fd43, 0d406FE00000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r52}, %fd44;
	}
	and.b32  	%r53, %r52, -2147483648;
	mov.f64 	%fd45, 0d3FE0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r54}, %fd45;
	}
	or.b32  	%r55, %r54, %r53;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r56, %temp}, %fd45;
	}
	mov.b64 	%fd46, {%r56, %r55};
	add.rz.f64 	%fd47, %fd44, %fd46;
	cvt.rzi.f64.f64 	%fd48, %fd47;
	cvt.rzi.s32.f64 	%r57, %fd48;
	st.global.u32 	[%rd1+4], %r57;
	mov.u32 	%r58, 0;
	st.global.u32 	[%rd1+8], %r58;
	bra.uni 	$L__BB0_13;

$L__BB0_2:
	setp.ge.f64 	%p5, %fd1, 0d3FF0C152382D7365;
	setp.lt.f64 	%p6, %fd1, 0d4000C152382D7365;
	and.pred  	%p7, %p5, %p6;
	@%p7 bra 	$L__BB0_11;
	bra.uni 	$L__BB0_3;

$L__BB0_11:
	mul.f64 	%fd34, %fd1, 0dC008000000000000;
	div.rn.f64 	%fd35, %fd34, 0d400921FB54442D18;
	add.f64 	%fd36, %fd35, 0d4000000000000000;
	mul.f64 	%fd37, %fd36, 0d406FE00000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r43}, %fd37;
	}
	and.b32  	%r44, %r43, -2147483648;
	mov.f64 	%fd38, 0d3FE0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r45}, %fd38;
	}
	or.b32  	%r46, %r45, %r44;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r47, %temp}, %fd38;
	}
	mov.b64 	%fd39, {%r47, %r46};
	add.rz.f64 	%fd40, %fd37, %fd39;
	cvt.rzi.f64.f64 	%fd41, %fd40;
	cvt.rzi.s32.f64 	%r48, %fd41;
	st.global.u32 	[%rd1], %r48;
	mov.u32 	%r49, 255;
	st.global.u32 	[%rd1+4], %r49;
	mov.u32 	%r50, 0;
	st.global.u32 	[%rd1+8], %r50;
	bra.uni 	$L__BB0_13;

$L__BB0_3:
	setp.ge.f64 	%p8, %fd1, 0d4000C152382D7365;
	setp.lt.f64 	%p9, %fd1, 0d400921FB54442D18;
	and.pred  	%p10, %p8, %p9;
	@%p10 bra 	$L__BB0_10;
	bra.uni 	$L__BB0_4;

$L__BB0_10:
	mov.u32 	%r35, 0;
	st.global.u32 	[%rd1], %r35;
	mov.u32 	%r36, 255;
	st.global.u32 	[%rd1+4], %r36;
	mul.f64 	%fd26, %fd1, 0d4008000000000000;
	div.rn.f64 	%fd27, %fd26, 0d400921FB54442D18;
	add.f64 	%fd28, %fd27, 0dC000000000000000;
	mul.f64 	%fd29, %fd28, 0d406FE00000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r37}, %fd29;
	}
	and.b32  	%r38, %r37, -2147483648;
	mov.f64 	%fd30, 0d3FE0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r39}, %fd30;
	}
	or.b32  	%r40, %r39, %r38;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r41, %temp}, %fd30;
	}
	mov.b64 	%fd31, {%r41, %r40};
	add.rz.f64 	%fd32, %fd29, %fd31;
	cvt.rzi.f64.f64 	%fd33, %fd32;
	cvt.rzi.s32.f64 	%r42, %fd33;
	st.global.u32 	[%rd1+8], %r42;
	bra.uni 	$L__BB0_13;

$L__BB0_4:
	setp.ge.f64 	%p11, %fd1, 0d400921FB54442D18;
	setp.lt.f64 	%p12, %fd1, 0d4010C152382D7365;
	and.pred  	%p13, %p11, %p12;
	@%p13 bra 	$L__BB0_9;
	bra.uni 	$L__BB0_5;

$L__BB0_9:
	mov.u32 	%r27, 0;
	st.global.u32 	[%rd1], %r27;
	mul.f64 	%fd18, %fd1, 0dC008000000000000;
	div.rn.f64 	%fd19, %fd18, 0d400921FB54442D18;
	add.f64 	%fd20, %fd19, 0d4010000000000000;
	mul.f64 	%fd21, %fd20, 0d406FE00000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r28}, %fd21;
	}
	and.b32  	%r29, %r28, -2147483648;
	mov.f64 	%fd22, 0d3FE0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r30}, %fd22;
	}
	or.b32  	%r31, %r30, %r29;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r32, %temp}, %fd22;
	}
	mov.b64 	%fd23, {%r32, %r31};
	add.rz.f64 	%fd24, %fd21, %fd23;
	cvt.rzi.f64.f64 	%fd25, %fd24;
	cvt.rzi.s32.f64 	%r33, %fd25;
	st.global.u32 	[%rd1+4], %r33;
	mov.u32 	%r34, 255;
	st.global.u32 	[%rd1+8], %r34;
	bra.uni 	$L__BB0_13;

$L__BB0_5:
	setp.ge.f64 	%p14, %fd1, 0d4010C152382D7365;
	setp.lt.f64 	%p15, %fd1, 0d4014F1A6C638D03F;
	and.pred  	%p16, %p14, %p15;
	@%p16 bra 	$L__BB0_8;
	bra.uni 	$L__BB0_6;

$L__BB0_8:
	mul.f64 	%fd10, %fd1, 0d4008000000000000;
	div.rn.f64 	%fd11, %fd10, 0d400921FB54442D18;
	add.f64 	%fd12, %fd11, 0dC010000000000000;
	mul.f64 	%fd13, %fd12, 0d406FE00000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r19}, %fd13;
	}
	and.b32  	%r20, %r19, -2147483648;
	mov.f64 	%fd14, 0d3FE0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r21}, %fd14;
	}
	or.b32  	%r22, %r21, %r20;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r23, %temp}, %fd14;
	}
	mov.b64 	%fd15, {%r23, %r22};
	add.rz.f64 	%fd16, %fd13, %fd15;
	cvt.rzi.f64.f64 	%fd17, %fd16;
	cvt.rzi.s32.f64 	%r24, %fd17;
	st.global.u32 	[%rd1], %r24;
	mov.u32 	%r25, 0;
	st.global.u32 	[%rd1+4], %r25;
	mov.u32 	%r26, 255;
	st.global.u32 	[%rd1+8], %r26;
	bra.uni 	$L__BB0_13;

$L__BB0_6:
	setp.ltu.f64 	%p17, %fd1, 0d4014F1A6C638D03F;
	setp.geu.f64 	%p18, %fd1, 0d401921FB54442D18;
	or.pred  	%p19, %p17, %p18;
	@%p19 bra 	$L__BB0_13;

	mov.u32 	%r11, 255;
	st.global.u32 	[%rd1], %r11;
	mov.u32 	%r12, 0;
	st.global.u32 	[%rd1+4], %r12;
	mul.f64 	%fd2, %fd1, 0dC008000000000000;
	div.rn.f64 	%fd3, %fd2, 0d400921FB54442D18;
	add.f64 	%fd4, %fd3, 0d4018000000000000;
	mul.f64 	%fd5, %fd4, 0d406FE00000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r13}, %fd5;
	}
	and.b32  	%r14, %r13, -2147483648;
	mov.f64 	%fd6, 0d3FE0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r15}, %fd6;
	}
	or.b32  	%r16, %r15, %r14;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r17, %temp}, %fd6;
	}
	mov.b64 	%fd7, {%r17, %r16};
	add.rz.f64 	%fd8, %fd5, %fd7;
	cvt.rzi.f64.f64 	%fd9, %fd8;
	cvt.rzi.s32.f64 	%r18, %fd9;
	st.global.u32 	[%rd1+8], %r18;

$L__BB0_13:
	setp.eq.s32 	%p20, %r4, -1;
	@%p20 bra 	$L__BB0_15;

	mul.lo.s32 	%r59, %r1, %r4;
	cvta.to.global.u64 	%rd10, %rd4;
	mul.wide.s32 	%rd11, %r59, 8;
	add.s64 	%rd12, %rd10, %rd11;
	ld.global.u32 	%r60, [%rd1];
	cvt.rn.f64.s32 	%fd49, %r60;
	ld.global.f64 	%fd50, [%rd12];
	mul.f64 	%fd51, %fd50, %fd49;
	cvt.rzi.s32.f64 	%r61, %fd51;
	st.global.u32 	[%rd1], %r61;
	ld.global.u32 	%r62, [%rd1+4];
	cvt.rn.f64.s32 	%fd52, %r62;
	mul.f64 	%fd53, %fd50, %fd52;
	cvt.rzi.s32.f64 	%r63, %fd53;
	st.global.u32 	[%rd1+4], %r63;
	ld.global.u32 	%r64, [%rd1+8];
	cvt.rn.f64.s32 	%fd54, %r64;
	mul.f64 	%fd55, %fd50, %fd54;
	cvt.rzi.s32.f64 	%r65, %fd55;
	st.global.u32 	[%rd1+8], %r65;

$L__BB0_15:
	ret;

}

