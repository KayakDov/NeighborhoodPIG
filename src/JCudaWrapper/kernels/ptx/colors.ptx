//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-32267302
// Cuda compilation tools, release 12.0, V12.0.140
// Based on NVVM 7.0.1
//

.version 8.0
.target sm_52
.address_size 64

	// .globl	colorsKernel

.visible .entry colorsKernel(
	.param .u32 colorsKernel_param_0,
	.param .u64 colorsKernel_param_1,
	.param .u32 colorsKernel_param_2,
	.param .u64 colorsKernel_param_3,
	.param .u32 colorsKernel_param_4,
	.param .u64 colorsKernel_param_5,
	.param .u32 colorsKernel_param_6
)
{
	.reg .pred 	%p<27>;
	.reg .b32 	%r<144>;
	.reg .f64 	%fd<131>;
	.reg .b64 	%rd<13>;


	ld.param.u32 	%r23, [colorsKernel_param_0];
	ld.param.u64 	%rd2, [colorsKernel_param_1];
	ld.param.u32 	%r20, [colorsKernel_param_2];
	ld.param.u64 	%rd3, [colorsKernel_param_3];
	ld.param.u32 	%r21, [colorsKernel_param_4];
	ld.param.u64 	%rd4, [colorsKernel_param_5];
	ld.param.u32 	%r22, [colorsKernel_param_6];
	mov.u32 	%r24, %ntid.x;
	mov.u32 	%r25, %ctaid.x;
	mov.u32 	%r26, %tid.x;
	mad.lo.s32 	%r1, %r25, %r24, %r26;
	setp.ge.s32 	%p1, %r1, %r23;
	@%p1 bra 	$L__BB0_27;

	cvta.to.global.u64 	%rd5, %rd2;
	mul.lo.s32 	%r27, %r1, %r20;
	mul.wide.s32 	%rd6, %r27, 8;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.f64 	%fd1, [%rd7];
	setp.eq.s32 	%p2, %r22, -1;
	mov.f64 	%fd3, 0d3FF0000000000000;
	@%p2 bra 	$L__BB0_3;

	mul.lo.s32 	%r28, %r1, %r22;
	cvta.to.global.u64 	%rd8, %rd4;
	mul.wide.s32 	%rd9, %r28, 8;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.f64 	%fd3, [%rd10];

$L__BB0_3:
	mul.lo.s32 	%r29, %r1, %r21;
	cvta.to.global.u64 	%rd11, %rd3;
	mul.wide.s32 	%rd12, %r29, 4;
	add.s64 	%rd1, %rd11, %rd12;
	setp.lt.f64 	%p3, %fd1, 0d3FF0C152382D7365;
	setp.ge.f64 	%p4, %fd1, 0d0000000000000000;
	and.pred  	%p5, %p4, %p3;
	@%p5 bra 	$L__BB0_24;
	bra.uni 	$L__BB0_4;

$L__BB0_24:
	setp.eq.s32 	%p26, %r21, 3;
	mul.f64 	%fd110, %fd1, 0d4008000000000000;
	div.rn.f64 	%fd111, %fd110, 0d400921FB54442D18;
	mul.f64 	%fd112, %fd111, 0d406FE00000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r125}, %fd112;
	}
	and.b32  	%r126, %r125, -2147483648;
	mov.f64 	%fd113, 0d3FE0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r127}, %fd113;
	}
	or.b32  	%r128, %r127, %r126;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r129, %temp}, %fd113;
	}
	mov.b64 	%fd114, {%r129, %r128};
	add.rz.f64 	%fd115, %fd112, %fd114;
	cvt.rzi.f64.f64 	%fd116, %fd115;
	cvt.rzi.s32.f64 	%r130, %fd116;
	mul.f64 	%fd117, %fd3, 0d406FE00000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r131}, %fd117;
	}
	and.b32  	%r132, %r131, -2147483648;
	or.b32  	%r133, %r127, %r132;
	mov.b64 	%fd118, {%r129, %r133};
	add.rz.f64 	%fd119, %fd117, %fd118;
	cvt.rzi.f64.f64 	%fd120, %fd119;
	cvt.rzi.s32.f64 	%r17, %fd120;
	cvt.rn.f64.s32 	%fd121, %r130;
	mul.f64 	%fd122, %fd3, %fd121;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r134}, %fd122;
	}
	and.b32  	%r135, %r134, -2147483648;
	or.b32  	%r136, %r127, %r135;
	mov.b64 	%fd123, {%r129, %r136};
	add.rz.f64 	%fd124, %fd122, %fd123;
	cvt.rzi.f64.f64 	%fd125, %fd124;
	cvt.rzi.s32.f64 	%r18, %fd125;
	mul.f64 	%fd126, %fd3, 0d0000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r137}, %fd126;
	}
	and.b32  	%r138, %r137, -2147483648;
	or.b32  	%r139, %r127, %r138;
	mov.b64 	%fd127, {%r129, %r139};
	add.rz.f64 	%fd128, %fd126, %fd127;
	cvt.rzi.f64.f64 	%fd129, %fd128;
	cvt.rzi.s32.f64 	%r19, %fd129;
	@%p26 bra 	$L__BB0_26;
	bra.uni 	$L__BB0_25;

$L__BB0_26:
	st.global.u32 	[%rd1], %r17;
	st.global.u32 	[%rd1+4], %r18;
	st.global.u32 	[%rd1+8], %r19;
	bra.uni 	$L__BB0_27;

$L__BB0_4:
	setp.ge.f64 	%p6, %fd1, 0d3FF0C152382D7365;
	setp.lt.f64 	%p7, %fd1, 0d4000C152382D7365;
	and.pred  	%p8, %p6, %p7;
	@%p8 bra 	$L__BB0_21;
	bra.uni 	$L__BB0_5;

$L__BB0_21:
	setp.eq.s32 	%p25, %r21, 3;
	mul.f64 	%fd89, %fd1, 0dC008000000000000;
	div.rn.f64 	%fd90, %fd89, 0d400921FB54442D18;
	add.f64 	%fd91, %fd90, 0d4000000000000000;
	mul.f64 	%fd92, %fd91, 0d406FE00000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r106}, %fd92;
	}
	and.b32  	%r107, %r106, -2147483648;
	mov.f64 	%fd93, 0d3FE0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r108}, %fd93;
	}
	or.b32  	%r109, %r108, %r107;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r110, %temp}, %fd93;
	}
	mov.b64 	%fd94, {%r110, %r109};
	add.rz.f64 	%fd95, %fd92, %fd94;
	cvt.rzi.f64.f64 	%fd96, %fd95;
	cvt.rzi.s32.f64 	%r111, %fd96;
	cvt.rn.f64.s32 	%fd97, %r111;
	mul.f64 	%fd98, %fd3, %fd97;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r112}, %fd98;
	}
	and.b32  	%r113, %r112, -2147483648;
	or.b32  	%r114, %r108, %r113;
	mov.b64 	%fd99, {%r110, %r114};
	add.rz.f64 	%fd100, %fd98, %fd99;
	cvt.rzi.f64.f64 	%fd101, %fd100;
	cvt.rzi.s32.f64 	%r14, %fd101;
	mul.f64 	%fd102, %fd3, 0d406FE00000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r115}, %fd102;
	}
	and.b32  	%r116, %r115, -2147483648;
	or.b32  	%r117, %r108, %r116;
	mov.b64 	%fd103, {%r110, %r117};
	add.rz.f64 	%fd104, %fd102, %fd103;
	cvt.rzi.f64.f64 	%fd105, %fd104;
	cvt.rzi.s32.f64 	%r15, %fd105;
	mul.f64 	%fd106, %fd3, 0d0000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r118}, %fd106;
	}
	and.b32  	%r119, %r118, -2147483648;
	or.b32  	%r120, %r108, %r119;
	mov.b64 	%fd107, {%r110, %r120};
	add.rz.f64 	%fd108, %fd106, %fd107;
	cvt.rzi.f64.f64 	%fd109, %fd108;
	cvt.rzi.s32.f64 	%r16, %fd109;
	@%p25 bra 	$L__BB0_23;
	bra.uni 	$L__BB0_22;

$L__BB0_23:
	st.global.u32 	[%rd1], %r14;
	st.global.u32 	[%rd1+4], %r15;
	st.global.u32 	[%rd1+8], %r16;
	bra.uni 	$L__BB0_27;

$L__BB0_25:
	shl.b32 	%r140, %r17, 16;
	shl.b32 	%r141, %r18, 8;
	or.b32  	%r142, %r141, %r140;
	or.b32  	%r143, %r142, %r19;
	st.global.u32 	[%rd1], %r143;
	bra.uni 	$L__BB0_27;

$L__BB0_5:
	setp.ge.f64 	%p9, %fd1, 0d4000C152382D7365;
	setp.lt.f64 	%p10, %fd1, 0d400921FB54442D18;
	and.pred  	%p11, %p9, %p10;
	@%p11 bra 	$L__BB0_18;
	bra.uni 	$L__BB0_6;

$L__BB0_18:
	setp.eq.s32 	%p24, %r21, 3;
	mul.f64 	%fd68, %fd1, 0d4008000000000000;
	div.rn.f64 	%fd69, %fd68, 0d400921FB54442D18;
	add.f64 	%fd70, %fd69, 0dC000000000000000;
	mul.f64 	%fd71, %fd70, 0d406FE00000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r87}, %fd71;
	}
	and.b32  	%r88, %r87, -2147483648;
	mov.f64 	%fd72, 0d3FE0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r89}, %fd72;
	}
	or.b32  	%r90, %r89, %r88;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r91, %temp}, %fd72;
	}
	mov.b64 	%fd73, {%r91, %r90};
	add.rz.f64 	%fd74, %fd71, %fd73;
	cvt.rzi.f64.f64 	%fd75, %fd74;
	cvt.rzi.s32.f64 	%r92, %fd75;
	mul.f64 	%fd76, %fd3, 0d0000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r93}, %fd76;
	}
	and.b32  	%r94, %r93, -2147483648;
	or.b32  	%r95, %r89, %r94;
	mov.b64 	%fd77, {%r91, %r95};
	add.rz.f64 	%fd78, %fd76, %fd77;
	cvt.rzi.f64.f64 	%fd79, %fd78;
	cvt.rzi.s32.f64 	%r11, %fd79;
	mul.f64 	%fd80, %fd3, 0d406FE00000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r96}, %fd80;
	}
	and.b32  	%r97, %r96, -2147483648;
	or.b32  	%r98, %r89, %r97;
	mov.b64 	%fd81, {%r91, %r98};
	add.rz.f64 	%fd82, %fd80, %fd81;
	cvt.rzi.f64.f64 	%fd83, %fd82;
	cvt.rzi.s32.f64 	%r12, %fd83;
	cvt.rn.f64.s32 	%fd84, %r92;
	mul.f64 	%fd85, %fd3, %fd84;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r99}, %fd85;
	}
	and.b32  	%r100, %r99, -2147483648;
	or.b32  	%r101, %r89, %r100;
	mov.b64 	%fd86, {%r91, %r101};
	add.rz.f64 	%fd87, %fd85, %fd86;
	cvt.rzi.f64.f64 	%fd88, %fd87;
	cvt.rzi.s32.f64 	%r13, %fd88;
	@%p24 bra 	$L__BB0_20;
	bra.uni 	$L__BB0_19;

$L__BB0_20:
	st.global.u32 	[%rd1], %r11;
	st.global.u32 	[%rd1+4], %r12;
	st.global.u32 	[%rd1+8], %r13;
	bra.uni 	$L__BB0_27;

$L__BB0_22:
	shl.b32 	%r121, %r14, 16;
	shl.b32 	%r122, %r15, 8;
	or.b32  	%r123, %r122, %r121;
	or.b32  	%r124, %r123, %r16;
	st.global.u32 	[%rd1], %r124;
	bra.uni 	$L__BB0_27;

$L__BB0_6:
	setp.ge.f64 	%p12, %fd1, 0d400921FB54442D18;
	setp.lt.f64 	%p13, %fd1, 0d4010C152382D7365;
	and.pred  	%p14, %p12, %p13;
	@%p14 bra 	$L__BB0_15;
	bra.uni 	$L__BB0_7;

$L__BB0_15:
	setp.eq.s32 	%p23, %r21, 3;
	mul.f64 	%fd47, %fd1, 0dC008000000000000;
	div.rn.f64 	%fd48, %fd47, 0d400921FB54442D18;
	add.f64 	%fd49, %fd48, 0d4010000000000000;
	mul.f64 	%fd50, %fd49, 0d406FE00000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r68}, %fd50;
	}
	and.b32  	%r69, %r68, -2147483648;
	mov.f64 	%fd51, 0d3FE0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r70}, %fd51;
	}
	or.b32  	%r71, %r70, %r69;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r72, %temp}, %fd51;
	}
	mov.b64 	%fd52, {%r72, %r71};
	add.rz.f64 	%fd53, %fd50, %fd52;
	cvt.rzi.f64.f64 	%fd54, %fd53;
	cvt.rzi.s32.f64 	%r73, %fd54;
	mul.f64 	%fd55, %fd3, 0d0000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r74}, %fd55;
	}
	and.b32  	%r75, %r74, -2147483648;
	or.b32  	%r76, %r70, %r75;
	mov.b64 	%fd56, {%r72, %r76};
	add.rz.f64 	%fd57, %fd55, %fd56;
	cvt.rzi.f64.f64 	%fd58, %fd57;
	cvt.rzi.s32.f64 	%r8, %fd58;
	cvt.rn.f64.s32 	%fd59, %r73;
	mul.f64 	%fd60, %fd3, %fd59;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r77}, %fd60;
	}
	and.b32  	%r78, %r77, -2147483648;
	or.b32  	%r79, %r70, %r78;
	mov.b64 	%fd61, {%r72, %r79};
	add.rz.f64 	%fd62, %fd60, %fd61;
	cvt.rzi.f64.f64 	%fd63, %fd62;
	cvt.rzi.s32.f64 	%r9, %fd63;
	mul.f64 	%fd64, %fd3, 0d406FE00000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r80}, %fd64;
	}
	and.b32  	%r81, %r80, -2147483648;
	or.b32  	%r82, %r70, %r81;
	mov.b64 	%fd65, {%r72, %r82};
	add.rz.f64 	%fd66, %fd64, %fd65;
	cvt.rzi.f64.f64 	%fd67, %fd66;
	cvt.rzi.s32.f64 	%r10, %fd67;
	@%p23 bra 	$L__BB0_17;
	bra.uni 	$L__BB0_16;

$L__BB0_17:
	st.global.u32 	[%rd1], %r8;
	st.global.u32 	[%rd1+4], %r9;
	st.global.u32 	[%rd1+8], %r10;
	bra.uni 	$L__BB0_27;

$L__BB0_19:
	shl.b32 	%r102, %r11, 16;
	shl.b32 	%r103, %r12, 8;
	or.b32  	%r104, %r103, %r102;
	or.b32  	%r105, %r104, %r13;
	st.global.u32 	[%rd1], %r105;
	bra.uni 	$L__BB0_27;

$L__BB0_7:
	setp.ge.f64 	%p15, %fd1, 0d4010C152382D7365;
	setp.lt.f64 	%p16, %fd1, 0d4014F1A6C638D03F;
	and.pred  	%p17, %p15, %p16;
	@%p17 bra 	$L__BB0_12;
	bra.uni 	$L__BB0_8;

$L__BB0_12:
	setp.eq.s32 	%p22, %r21, 3;
	mul.f64 	%fd26, %fd1, 0d4008000000000000;
	div.rn.f64 	%fd27, %fd26, 0d400921FB54442D18;
	add.f64 	%fd28, %fd27, 0dC010000000000000;
	mul.f64 	%fd29, %fd28, 0d406FE00000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r49}, %fd29;
	}
	and.b32  	%r50, %r49, -2147483648;
	mov.f64 	%fd30, 0d3FE0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r51}, %fd30;
	}
	or.b32  	%r52, %r51, %r50;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r53, %temp}, %fd30;
	}
	mov.b64 	%fd31, {%r53, %r52};
	add.rz.f64 	%fd32, %fd29, %fd31;
	cvt.rzi.f64.f64 	%fd33, %fd32;
	cvt.rzi.s32.f64 	%r54, %fd33;
	cvt.rn.f64.s32 	%fd34, %r54;
	mul.f64 	%fd35, %fd3, %fd34;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r55}, %fd35;
	}
	and.b32  	%r56, %r55, -2147483648;
	or.b32  	%r57, %r51, %r56;
	mov.b64 	%fd36, {%r53, %r57};
	add.rz.f64 	%fd37, %fd35, %fd36;
	cvt.rzi.f64.f64 	%fd38, %fd37;
	cvt.rzi.s32.f64 	%r5, %fd38;
	mul.f64 	%fd39, %fd3, 0d0000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r58}, %fd39;
	}
	and.b32  	%r59, %r58, -2147483648;
	or.b32  	%r60, %r51, %r59;
	mov.b64 	%fd40, {%r53, %r60};
	add.rz.f64 	%fd41, %fd39, %fd40;
	cvt.rzi.f64.f64 	%fd42, %fd41;
	cvt.rzi.s32.f64 	%r6, %fd42;
	mul.f64 	%fd43, %fd3, 0d406FE00000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r61}, %fd43;
	}
	and.b32  	%r62, %r61, -2147483648;
	or.b32  	%r63, %r51, %r62;
	mov.b64 	%fd44, {%r53, %r63};
	add.rz.f64 	%fd45, %fd43, %fd44;
	cvt.rzi.f64.f64 	%fd46, %fd45;
	cvt.rzi.s32.f64 	%r7, %fd46;
	@%p22 bra 	$L__BB0_14;
	bra.uni 	$L__BB0_13;

$L__BB0_14:
	st.global.u32 	[%rd1], %r5;
	st.global.u32 	[%rd1+4], %r6;
	st.global.u32 	[%rd1+8], %r7;
	bra.uni 	$L__BB0_27;

$L__BB0_16:
	shl.b32 	%r83, %r8, 16;
	shl.b32 	%r84, %r9, 8;
	or.b32  	%r85, %r84, %r83;
	or.b32  	%r86, %r85, %r10;
	st.global.u32 	[%rd1], %r86;
	bra.uni 	$L__BB0_27;

$L__BB0_8:
	setp.ltu.f64 	%p18, %fd1, 0d4014F1A6C638D03F;
	setp.geu.f64 	%p19, %fd1, 0d401921FB54442D18;
	or.pred  	%p20, %p18, %p19;
	@%p20 bra 	$L__BB0_27;

	setp.eq.s32 	%p21, %r21, 3;
	mul.f64 	%fd5, %fd1, 0dC008000000000000;
	div.rn.f64 	%fd6, %fd5, 0d400921FB54442D18;
	add.f64 	%fd7, %fd6, 0d4018000000000000;
	mul.f64 	%fd8, %fd7, 0d406FE00000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r30}, %fd8;
	}
	and.b32  	%r31, %r30, -2147483648;
	mov.f64 	%fd9, 0d3FE0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r32}, %fd9;
	}
	or.b32  	%r33, %r32, %r31;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r34, %temp}, %fd9;
	}
	mov.b64 	%fd10, {%r34, %r33};
	add.rz.f64 	%fd11, %fd8, %fd10;
	cvt.rzi.f64.f64 	%fd12, %fd11;
	cvt.rzi.s32.f64 	%r35, %fd12;
	mul.f64 	%fd13, %fd3, 0d406FE00000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r36}, %fd13;
	}
	and.b32  	%r37, %r36, -2147483648;
	or.b32  	%r38, %r32, %r37;
	mov.b64 	%fd14, {%r34, %r38};
	add.rz.f64 	%fd15, %fd13, %fd14;
	cvt.rzi.f64.f64 	%fd16, %fd15;
	cvt.rzi.s32.f64 	%r2, %fd16;
	mul.f64 	%fd17, %fd3, 0d0000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r39}, %fd17;
	}
	and.b32  	%r40, %r39, -2147483648;
	or.b32  	%r41, %r32, %r40;
	mov.b64 	%fd18, {%r34, %r41};
	add.rz.f64 	%fd19, %fd17, %fd18;
	cvt.rzi.f64.f64 	%fd20, %fd19;
	cvt.rzi.s32.f64 	%r3, %fd20;
	cvt.rn.f64.s32 	%fd21, %r35;
	mul.f64 	%fd22, %fd3, %fd21;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r42}, %fd22;
	}
	and.b32  	%r43, %r42, -2147483648;
	or.b32  	%r44, %r32, %r43;
	mov.b64 	%fd23, {%r34, %r44};
	add.rz.f64 	%fd24, %fd22, %fd23;
	cvt.rzi.f64.f64 	%fd25, %fd24;
	cvt.rzi.s32.f64 	%r4, %fd25;
	@%p21 bra 	$L__BB0_11;
	bra.uni 	$L__BB0_10;

$L__BB0_11:
	st.global.u32 	[%rd1], %r2;
	st.global.u32 	[%rd1+4], %r3;
	st.global.u32 	[%rd1+8], %r4;
	bra.uni 	$L__BB0_27;

$L__BB0_13:
	shl.b32 	%r64, %r5, 16;
	shl.b32 	%r65, %r6, 8;
	or.b32  	%r66, %r65, %r64;
	or.b32  	%r67, %r66, %r7;
	st.global.u32 	[%rd1], %r67;
	bra.uni 	$L__BB0_27;

$L__BB0_10:
	shl.b32 	%r45, %r2, 16;
	shl.b32 	%r46, %r3, 8;
	or.b32  	%r47, %r46, %r45;
	or.b32  	%r48, %r47, %r4;
	st.global.u32 	[%rd1], %r48;

$L__BB0_27:
	ret;

}

