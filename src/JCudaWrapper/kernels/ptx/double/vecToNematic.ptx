//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-32267302
// Cuda compilation tools, release 12.0, V12.0.140
// Based on NVVM 7.0.1
//

.version 8.0
.target sm_52
.address_size 64

	// .globl	vecToNematicKernel

.visible .entry vecToNematicKernel(
	.param .u32 vecToNematicKernel_param_0,
	.param .u64 vecToNematicKernel_param_1,
	.param .u32 vecToNematicKernel_param_2,
	.param .u32 vecToNematicKernel_param_3,
	.param .u64 vecToNematicKernel_param_4,
	.param .u32 vecToNematicKernel_param_5,
	.param .u32 vecToNematicKernel_param_6
)
{
	.reg .pred 	%p<6>;
	.reg .b32 	%r<19>;
	.reg .f64 	%fd<14>;
	.reg .b64 	%rd<9>;


	ld.param.u32 	%r6, [vecToNematicKernel_param_0];
	ld.param.u64 	%rd5, [vecToNematicKernel_param_1];
	ld.param.u32 	%r2, [vecToNematicKernel_param_2];
	ld.param.u32 	%r3, [vecToNematicKernel_param_3];
	ld.param.u64 	%rd6, [vecToNematicKernel_param_4];
	ld.param.u32 	%r4, [vecToNematicKernel_param_5];
	ld.param.u32 	%r5, [vecToNematicKernel_param_6];
	cvta.to.global.u64 	%rd1, %rd5;
	cvta.to.global.u64 	%rd2, %rd6;
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %ctaid.x;
	mov.u32 	%r9, %tid.x;
	mad.lo.s32 	%r1, %r8, %r7, %r9;
	setp.ge.s32 	%p1, %r1, %r6;
	@%p1 bra 	$L__BB0_8;

	mul.lo.s32 	%r10, %r1, 3;
	div.s32 	%r11, %r10, %r3;
	mul.lo.s32 	%r12, %r11, %r3;
	sub.s32 	%r13, %r10, %r12;
	mad.lo.s32 	%r14, %r11, %r2, %r13;
	mul.wide.s32 	%rd7, %r14, 8;
	add.s64 	%rd3, %rd1, %rd7;
	div.s32 	%r15, %r10, %r5;
	mul.lo.s32 	%r16, %r15, %r5;
	sub.s32 	%r17, %r10, %r16;
	mad.lo.s32 	%r18, %r15, %r4, %r17;
	ld.global.f64 	%fd1, [%rd3+8];
	setp.lt.f64 	%p2, %fd1, 0d0000000000000000;
	mul.wide.s32 	%rd8, %r18, 8;
	add.s64 	%rd4, %rd2, %rd8;
	@%p2 bra 	$L__BB0_6;
	bra.uni 	$L__BB0_2;

$L__BB0_6:
	ld.global.f64 	%fd13, [%rd3];
	bra.uni 	$L__BB0_7;

$L__BB0_2:
	setp.neu.f64 	%p3, %fd1, 0d0000000000000000;
	@%p3 bra 	$L__BB0_4;

	ld.global.f64 	%fd13, [%rd3];
	setp.lt.f64 	%p4, %fd13, 0d0000000000000000;
	@%p4 bra 	$L__BB0_7;
	bra.uni 	$L__BB0_4;

$L__BB0_7:
	neg.f64 	%fd8, %fd13;
	st.global.f64 	[%rd4], %fd8;
	ld.global.f64 	%fd9, [%rd3+8];
	neg.f64 	%fd10, %fd9;
	st.global.f64 	[%rd4+8], %fd10;
	ld.global.f64 	%fd11, [%rd3+16];
	neg.f64 	%fd12, %fd11;
	st.global.f64 	[%rd4+16], %fd12;
	bra.uni 	$L__BB0_8;

$L__BB0_4:
	setp.eq.s64 	%p5, %rd2, %rd1;
	@%p5 bra 	$L__BB0_8;

	ld.global.f64 	%fd5, [%rd3];
	st.global.f64 	[%rd4], %fd5;
	ld.global.f64 	%fd6, [%rd3+8];
	st.global.f64 	[%rd4+8], %fd6;
	ld.global.f64 	%fd7, [%rd3+16];
	st.global.f64 	[%rd4+16], %fd7;

$L__BB0_8:
	ret;

}

